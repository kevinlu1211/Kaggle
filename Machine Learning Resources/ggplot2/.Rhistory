lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimateY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
fit1 <- glmnet(Z, y, alpha = 0)
dim(fit1)$beta
dim(fit1$beta)
fit1$beta[1,]
fit1$beta[,1]
dim(fit$lambda)
length(fit$lambda)
lambda
dim(fit1$beta)
fit1 <- glmnet(Z, y, alpha = 0, nlambda=10)
dim(fit1$beta)
fit1 <- glmnet(Z, y, alpha = 0, nlambda=1)
dim(fit1$beta)
fit1$beta[,1]
which(fit1$beta[,1] > 1e-5)
which(fit1$beta[,1] > 1e-10)
fit1 <- glmnet(Z, y, alpha = 0)
dim(fit1$beta)
dim(fit1$beta)
fit1$beta[1,1]
fit1$beta[1,2]
fit1$beta[2,1]
fit1$finalModel
fit1 <- glmnet(Z, y, alpha = 0)
fit1$finalModel
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
estimatedNewY2 <- Z %*% mx1$ / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# estimating Y using glmnet
fit1 <- glmnet(Z, y, alpha = 0)
fit1$finalModel
dim(fit1$beta)
fit1$beta[1,1]
lambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("New method2:", estimatedNewY2[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
?mixed.solve
x = NULL
if (x == NULL) {print("hi")}
if (x == NULL) {print("hi")}
x = 1
if (x) {print("hi")}
x = NULL
if (x) {print("hi")}
is.null
is.null(x)
Z <- matrix(1:4,2,2)
Z
y <- matrix(1:2,2,1)
y
createTrainTest <- function(Z, y) {
n <- nrow(Z)
# shuffle the data
Z <- Z[sample(n),]
y <- y[sample(n),]
endIndex <- as.integer(n * 0.7)
# create the train and test set
train_Z <- Z[1:endIndex,]
test_Z <- Z[-(1:endIndex),]
train_y <- y[1:endIndex,]
test_y <- y[-(1:endIndex),]
list(train_Z = train_Z, test_Z = test_Z, train_y = train_y, test_y = test_y)
}
createTrainTest(Z,y)
y <- matrix(1:5,5,1)
Z <- matrix(1:10,2,5)
Z
Z <- matrix(1:25,5,5)
Z
y
createTrainTest(Z,y)
createTrainTest <- function(Z, y) {
n <- nrow(Z)
# shuffle the data
Z <- Z[sample(n),]
y <- y[sample(n),]
endIndex <- as.integer(n * 0.7)
print(endIndex)
# create the train and test set
train_Z <- Z[1:endIndex,]
test_Z <- Z[-(1:endIndex),]
train_y <- y[1:endIndex,]
test_y <- y[-(1:endIndex),]
list(train_Z = train_Z, test_Z = test_Z, train_y = train_y, test_y = test_y)
}
createTrainTest(Z,y)
y[1:3,]
y
y <- y[sample(5)]
h
y
y <- y[sample(5),]
y
y <- matrix(1:5,5,1)
y
y <- y[sample(5),]
y
y[1:3,]
Z
Z[sample(5),]
Z[sample(5),]
Z[sample(5),]
Z[sample(5),]
Z[sample(5),]
n = 5
y <- matrix(y[sample(n),], n, 1)
y[sample(n),]
y
y <- matrix(1:5,5,1)
y <- matrix(y[sample(n),], n, 1)
y
Z
y
createTrainTest(Z,y)
createTrainTest <- function(Z, y) {
n <- nrow(Z)
# shuffle the data
Z <- Z[sample(n),]
y <- matrix(y[sample(n),], n, 1)
endIndex <- as.integer(n * 0.8)
# create the train and test set
train_Z <- Z[1:endIndex,]
test_Z <- Z[-(1:endIndex),]
train_y <- y[1:endIndex,]
test_y <- y[-(1:endIndex),]
list(train_Z = train_Z, test_Z = test_Z, train_y = train_y, test_y = test_y)
}
createTrainTest(Z,y)
data <- loadData("test")
library(plink2R)
library(plink2R)
data <- loadData("test")
### GGPLOT2 TUTORIAL ###
setwd("/Users/kevinlu/Documents/Data Science/Kaggle/ggplot2")
housing <- read.csv("dataSets/landdata-states.csv")
str(housing)
### INTRODUCTION ###
# using default package to plot a histogram
hist(housing$Home.Value)
# now use ggplot2
library(ggplot2)
ggplot(housing, aes(x = Home.Value)) + geom_histogram()
# using default package for color scatter plot
plot(Home.Value ~ Date, data = subset(housing, State == "MA"))
points(Home.Value ~ Date, col = "red", data = subset(housing, State == "TX"))
# pch changes the icon for the legend
legend(19750, 400000, c("MA", "TX"), title = "State", col = c("black", "red"), pch=c(1,1))
# now use ggplot2
ggplot(subset(housing, State %in% c("MA", "TX")), aes(x=Date, y = Home.Value, color = State)) + geom_point()
### GEOMETRIC OBJECTS AND AESTHETICS ###
### Points (Scatterplot) ###
# use ggplot2 to plot the 2001 q1 housing data
hp2001Q1 <- subset(housing, Date == 20011)
ggplot(hp2001Q1, aes(y=Structure.Cost, x = Land.Value)) + geom_point()
# create a new column in the data frame of hp2001Q1
hp2001Q1$pred.SC <- predict(lm(Structure.Cost ~ Land.Value, data = hp2001Q1))
### Lines (Prediction Line) ###
p1 <- ggplot(hp2001Q1, aes(x=Land.Value, y=Structure.Cost))
p1 + geom_point(aes(color=Home.Value)) + geom_line(aes(y=pred.SC))
# geom_smooth() is predicted value of y given x
p1 + geom_point(aes(color=Home.Value)) + geom_smooth()
# Note here that if only geom_line() is used it will connect the points of geom_point
p2 <- ggplot(hp2001Q1, aes(x=Land.Value, y=Home.Value))
p2 + geom_point(size = 3) + geom_line()
# to make a prediction line do geom_line(aes(y=...))
p2 + geom_point(size = 3) + geom_line(aes(y = 1))
# note that size = 1 is needed here as ggplot defaults to size = 3
p2 + geom_point(size = 3) + geom_line(aes(y=pred.SC), size = 1)
str(hp2001Q1)
### Text (Label Points) ###
library(ggrepel)
p1 + geom_text(aes(label=State), size = 3)
p1 +
geom_point() +
geom_text_repel(aes(label=State), size = 3)
### Aesthetic Mapping VS Assignment ###
# Note that aesthetics that are specific to the geom should be in geom_...(aes(size = 3)), instead of being inside the ggplot function
p2 <- ggplot(hp2001Q1, aes(x=Land.Value, y=Structure.Cost, size = 3))
# note that ggplot(hp2001Q1, aes(x=Land.Value, y=Structure.Cost), size = 3) would have no effect as well
p2 + geom_point(aes(color=Home.Value)) # note how we have an extra icon in the legend from the size = 3 in p2
# Correct way add the size specific to each geom outside of the ggplot and in the geom_... function
p3 <- ggplot(hp2001Q1, aes(x=Land.Value, y=Structure.Cost))
p3 + geom_point(color = "red", size = 10) # this is the correct way
# we could even give them size and color according to their features
p3 + geom_point(aes(size=Land.Value, color = Structure.Cost))
p1 + geom_point(aes(color=Home.Value, shape = region))
data(iris)
irs
iris
data(iris)
dat <- as.matrix(iris[,-5])
pca <- prcomp(dat, retx=TRUE, center=TRUE, scale=TRUE)
###Create new data sets for each of the three species.
#Biometric values are based on the distributions of the original data means
pca
pca$x
dat
pca
setosa.mean <- apply(iris[iris$Species=="setosa",-5], 2, mean)
setosa.cov <- cov(iris[iris$Species=="setosa",-5])
iris[iris$Species=="setosa",-5]
iris
iris[iris$Species=="setosa",]
iris[iris$Species=="setosa",-5]
setosa.mean
setosa.cov <- cov(iris[iris$Species=="setosa",-5])
setosa.cov
set.seed(1)
mvrnorm
?mvrnorm
require(MASS)
?mvrnorm
new.setosa <- mvrnorm(n, setosa.mean, setosa.cov)
new.versicolor <- mvrnorm(n, versicolor.mean, versicolor.cov)
new.virginica <- mvrnorm(n, virginica.mean, virginica.cov)
versicolor.mean <- apply(iris[iris$Species=="versicolor",-5], 2, mean)
versicolor.cov <- cov(iris[iris$Species=="versicolor",-5])
virginica.mean <- apply(iris[iris$Species=="virginica",-5], 2, mean)
virginica.cov <- cov(iris[iris$Species=="virginica",-5])
summary(pca)
