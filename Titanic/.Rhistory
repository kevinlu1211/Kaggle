ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimateY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
fit1 <- glmnet(Z, y, alpha = 0)
dim(fit1)$beta
dim(fit1$beta)
fit1$beta[1,]
fit1$beta[,1]
dim(fit$lambda)
length(fit$lambda)
lambda
dim(fit1$beta)
fit1 <- glmnet(Z, y, alpha = 0, nlambda=10)
dim(fit1$beta)
fit1 <- glmnet(Z, y, alpha = 0, nlambda=1)
dim(fit1$beta)
fit1$beta[,1]
which(fit1$beta[,1] > 1e-5)
which(fit1$beta[,1] > 1e-10)
fit1 <- glmnet(Z, y, alpha = 0)
dim(fit1$beta)
dim(fit1$beta)
fit1$beta[1,1]
fit1$beta[1,2]
fit1$beta[2,1]
fit1$finalModel
fit1 <- glmnet(Z, y, alpha = 0)
fit1$finalModel
# mixed.solve testing
library(glmnet)
library(rrBLUP)
library(MASS)
scale2 <- function(X) {
p <- colSums(X)/(2 * nrow(X))
sweep(
sweep(X, 2, 2 * p, "-"), 2, sqrt(2*p * (1-p)), "/"
)
}
make.data <- function(n,m,maf,h2,u=NULL,scale=TRUE) {
pr <- c((1 - maf)^2, 2 * maf * (1 - maf), maf^2)
M <- matrix(sample(0:2, n * m, replace=TRUE, prob=pr), n, m)
if(scale) {
M <- scale2(M)
}
if(is.null(u)) {
u<-rnorm(m)
}
g <- M %*% u
e <- rnorm(n, mean=0, sd=sqrt((1 - h2) / h2 * var(g)))
y <- g + e
y <- scale(y)
rownames(M) <- 1:nrow(M)
list(X=M, y=y, u=u,varE=var(e),varG=var(g))
}
maf <- 0.05
h2 <- 0.5
n <- 250
m <- 5000
r <- make.data(n=n, m=m, maf=maf, h2=h2, scale=FALSE)
X <- r$X
y <- r$y
Z <- scale2(X)
# Genetic similarity matrix using {0, 1, 2}
ZZt <- tcrossprod(Z) / m
# Genetic similarity matrix using {-1, 0, 1}
K <- A.mat(X - 1)
mx1 <- mixed.solve(y=y, K=A.mat(X - 1))
mx2 <- mixed.solve(y=y, K=K)
mx3 <- mixed.solve(y=y, Z=Z / sqrt(m))
(h2.v1 <- with(mx1, Vu / (Vu + Ve)))
(h2.v2 <- with(mx2, Vu / (Vu + Ve)))
(h2.v3 <- with(mx3, Vu / (Vu + Ve)))
dim(mx3$u)
dim(mx2$u)
dim(Z)
# estimating Y using our new RR
effectSizeNew <- mx3$u
estimatedNewY <- Z %*% mx3$u / sqrt(m)
estimatedNewY2 <- Z %*% mx1$ / sqrt(m)
# # estimating Y using lm.ridge
lambda <- as.numeric(r$varE/r$varG)
fit <- lm.ridge(y ~ Z, lambda = c(lambda))
length(fit$coef)
effectSizesLambda <- fit$coef
estimatedY <- Z %*% effectSizesLambda + mean(y)
# estimating Y using glmnet
fit1 <- glmnet(Z, y, alpha = 0)
fit1$finalModel
dim(fit1$beta)
fit1$beta[1,1]
lambda
# Using vanilla ridge regression, but it is too slow due to matrix inversion
# lambda <- as.numeric(r$varE/r$varG)
# effectSizeOld <- solve(((t(Z) %*% Z)) + lambda * diag(m)) %*% t(Z) %*% y
# estimatedOldY <- Z %*% effectSizeOld + mean(y)
# estimating Y using rrBLUP
data <- data.frame(y=y, gid=1:n)
estimatedMixedY <- mixed.solve(y, K=K, SE=TRUE)$u
estimatedKinY <- kin.blup(data = data, geno="gid",pheno="y", K=K)$g
# print(paste("Old method:",estimatedOldY[1]))
print(paste("lm.ridge method:", estimatedY[1]))
print(paste("New method:", estimatedNewY[1]))
print(paste("New method2:", estimatedNewY2[1]))
print(paste("mixed.solve:", estimatedMixedY[1]))
print(paste("kin.blup:",estimatedKinY[1]))
?mixed.solve
# Titanic Kaggle
# import the libraries that are used
library(rpart)
library(randomForest)
library(e1071)
# First get the train and test set
setwd("Documents/Data Science/Kaggle/Titanic")
train <- read.csv("~/Documents/Data Science/Kaggle/Titanic/train.csv")
test <- read.csv("~/Documents/Data Science/Kaggle/Titanic/test.csv")
### DATA CLEANING & FEATURE ENGINEERING ###
# First lets investigate the factors and maybe engineer some new features for prediction
str(train)
str(test)
# need to make the data frames have the same column to trainTest them
test$Survived <- 0
trainTest <- rbind(train,test)
str(trainTest)
# See that some extra information can be extracted from the data, such as the title of the passenger
# Maybe the title of the passenger will be useful? Regardless let's create it as a new feature
trainTest$Title <- sapply(as.character(trainTest$Name), FUN = function(x) {strsplit(x, split = '[,.]')[[1]][2]})
trainTest$Title <- sub(' ', '', trainTest$Title)
# See that there are many "rare" titles some of which are associated with people, aggregate these titles to avoid overfitting if the
# feature is actually used
table(trainTest$Title)
# now aggregate some of the rare title values
trainTest$Title[trainTest$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
trainTest$Title[trainTest$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
trainTest$Title[trainTest$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
table(trainTest$Title)
trainTest$title <- NULL
# Make a fare factor column
summary(trainTest$Fare)
trainTest$FareFactor <- "0"
# As there is one person with NA for fare it will be imputed with a decision tree
dt.fare <- rpart(Fare ~ Pclass + Sex + SibSp + Parch + FareFactor + Embarked + Title + Age ,
data=trainTest[!is.na(trainTest$Fare),],
method="anova")
trainTest$Fare[is.na(trainTest$Fare)] <- predict(dt.fare, trainTest[is.na(trainTest$Fare),])
# From the Kaggle website we can see that SibSp means the number of siblings/spouses on board, and Parch is the number of parents/children on board
# So create a new feature which is a family ID that records the number of total family members they have on board and the family name
str(trainTest)
trainTest$FamilySize <- trainTest$SibSp + trainTest$Parch + 1
# create new familyID
trainTest$Surname <- sapply(as.character(trainTest$Name), FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})
trainTest$FamilyID <- paste(as.character(trainTest$FamilySize), trainTest$Surname, sep="")
trainTest$FamilyID[trainTest$FamilySize<=2] <-'Small'
# there are some families that have an ID of 3FamilyName, but only 1 occurance in the table, it might be because
# people in the same family don't have the same surnames, but we ignore this fact
table(trainTest$FamilyID)
famIDs <- data.frame(table(trainTest$FamilyID))
# get all the small families by frequency calculated from SibSp + Parch + 1
famIDs <- famIDs[famIDs$Freq <=2,]
# make these families small as well, although we are losing information by disregarding their family members
trainTest$FamilyID[trainTest$FamilyID %in% famIDs$Var1] <- 'Small'
trainTest$FamilyID <- factor(trainTest$FamilyID)
# Make 3 factor levels to capture the minimum ~ 1st quartile, 1st ~ 3rd quartile, 3rd quartile ~ max
trainTest$FareFactor[trainTest$Fare <= 7.896] <- "Low"
trainTest$FareFactor[trainTest$Fare > 7.896 & trainTest$Fare <= 31.28] <- "Medium"
trainTest$FareFactor[trainTest$Fare > 31.28] <- "High"
# As we know that children are put onto the lifeboats first create a new feature for this
trainTest$Child <- 0
trainTest$Child[trainTest$Age < 18] <- 1
which(!complete.cases(trainTest))
str(trainTest)
# See that we have some NA's for age, no problem we can impute these values by using a decision tree
dt.age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + FareFactor + FamilyID + Embarked + Title + Child,
data=trainTest[!is.na(trainTest$Age),],
method="anova")
trainTest$Age[is.na(trainTest$Age)] <- predict(dt.age, trainTest[is.na(trainTest$Age),])
# See that there are no incomplete cases now so proceed to split the data into train and set
which(!complete.cases(trainTest))
# get the newTrain data
df <- data.frame(trainTest)
df$FamilySize <- as.factor(df$FamilySize)
train <- df[1:891,]
test <- df[-(1:891),]
### LOGISTIC REGRESSION ###
# # Perform 10 fold cross validation to select causal variables
# crossValidate <- function(k = 10, data, folds) {
#   accuracies <- c()
#   for(i in 1:k){
#     #Segment data by fold using the which() function
#     testIndexes <- which(folds==i,arr.ind=TRUE)
#     trainData <- data[-testIndexes, ]
#     testData <- data[testIndexes, ]
#     accuracy <- logistic.regression.accuracy(train,test)
#     accuracies <- c(accuracies, accuracy)
#   }
#   print(accuracies)
#   return(mean(accuracies))
# }
logistic.regression.accuracy <- function(test, fit) {
actual <- array(test[,"Survived"])
predicted <- predict(fit, test, type = "response")
predicted[predicted < 0.5] <- 0
predicted[predicted >= 0.5] <- 1
correctCount <- 0
for(i in 1:length(predicted)) {
if(predicted[i] == actual[i]) {
correctCount <- correctCount + 1
}
}
accuracy <- correctCount/nrow(test)
return(accuracy)
}
folds <- cut(seq(1,nrow(df[1:891,])),breaks=10,labels=FALSE)
# First test which factors are causal so we know what to use in our final model
fit <- glm(Survived ~ Pclass + Sex + Age + Child + FamilySize, data = train, family = "binomial")
fit1 <- step(fit)
# Hence we can determine that Pclass + Sex + Age + SibSp are the causal ones
summary(fit1)
print(logistic.regression.accuracy(test, fit1))
summary(fit1)
predicted <- predict(fit1, df[-(1:891),], type = "response")
predicted
predicted[predicted < 0.5] <- 0
predicted[predicted >= 0.5] <- 1
submit <- data.frame(PassengerId = test$PassengerId, Survived = predicted)
submit
write.csv(submit, file = "logisticRegression3.R", row.names = F)
fit <- glm(Survived ~ Pclass + Sex + Child + FamilySize, data = train, family = "binomial")
fit1 <- step(fit)
# Hence we can determine that Pclass + Sex + Age + SibSp are the causal ones
summary(fit1)
print(logistic.regression.accuracy(test, fit1))
str(train)
fit <- glm(Survived ~ Pclass + Sex + Child + FamilySize + FareFactor, data = train, family = "binomial")
fit1 <- step(fit)
# Hence we can determine that Pclass + Sex + Age + SibSp are the causal ones
summary(fit1)
str(train)
print(logistic.regression.accuracy(test, fit1))
fit <- glm(Survived ~ Pclass + Sex + Child + FamilySize + FareFactor + Title, data = train, family = "binomial")
fit1 <- step(fit)
# Hence we can determine that Pclass + Sex + Age + SibSp are the causal ones
summary(fit1)
str(train)
print(logistic.regression.accuracy(test, fit1))
fit <- glm(Survived ~ Pclass + Sex + Child, data = train, family = "binomial")
summary(fit)
fit1 <- step(fit)
# Hence we can determine that Pclass + Sex + Age + SibSp are the causal ones
summary(fit1)
print(logistic.regression.accuracy(test, fit1))
train <- df[1:891,]
test <- df[-(1:891),]
logistic.regression.accuracy <- function(test, fit) {
actual <- array(test[,"Survived"])
predicted <- predict(fit, test, type = "response")
predicted[predicted < 0.5] <- 0
predicted[predicted >= 0.5] <- 1
correctCount <- 0
for(i in 1:length(predicted)) {
if(predicted[i] == actual[i]) {
correctCount <- correctCount + 1
}
}
print("Number of correct classifications:")
print(correctCount)
print("Number of samples:")
print(nrow(test))
accuracy <- correctCount/nrow(test)
return(accuracy)
}
fit <- glm(Survived ~ Pclass + Sex + Child, data = train, family = "binomial")
summary(fit)
print(logistic.regression.accuracy(test, fit1))
